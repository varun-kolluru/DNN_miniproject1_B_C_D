{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87760caa",
   "metadata": {},
   "source": [
    "### Invariance property of CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varun/Documents/ML_work/Kaggle/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/varun/Documents/ML_work/Kaggle/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/varun/Documents/ML_work/Kaggle/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "small_shift: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - small_shift: 80.71% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - flip: 88.25% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - noise: 42.16% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - rotate_10: 44.19% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - rotate_20: 30.94% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - rotate_30: 22.7% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - rotate_45: 13.32% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_0.8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - scale_0.8: 46.68% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_1.2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - scale_1.2: 80.21% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_1.5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - scale_1.5: 48.49% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "small_shift: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - small_shift: 62.97% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 28.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - flip: 90.07% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - noise: 54.07% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - rotate_10: 45.89% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 28.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - rotate_20: 31.44% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - rotate_30: 21.61% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rotate_45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - rotate_45: 19.71% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_0.8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 27.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - scale_0.8: 16.75% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_1.2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - scale_1.2: 79.66% similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scale_1.5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 26.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet - scale_1.5: 60.23% similarity\n",
      "\n",
      "ðŸ“Š Invariance Similarity Summary (Cosine %):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----- 1. Load pretrained models -----\n",
    "torch.hub.set_dir(os.path.abspath(\"resnet50\"))\n",
    "resnet50 = models.resnet50(pretrained=True).to(device).eval()\n",
    "\n",
    "torch.hub.set_dir(os.path.abspath(\"alexnet\"))\n",
    "alexnet = models.alexnet(pretrained=True).to(device).eval()\n",
    "models_dict = {'ResNet50': resnet50, 'AlexNet': alexnet}\n",
    "\n",
    "# ----- 2. Dataset (100 random synthetic images) -----\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ----- 2. Dataset (CIFAR-100, only 100 images) -----\n",
    "cifar_root = os.path.abspath(\"./cifar100_data\")\n",
    "\n",
    "# Download CIFAR-100 dataset into current directory if not present\n",
    "full_dataset = datasets.CIFAR100(\n",
    "    root=cifar_root,\n",
    "    train=False,          # use test split for convenience\n",
    "    download=False,\n",
    "    transform=base_transform\n",
    ")\n",
    "\n",
    "# Take only first 100 samples\n",
    "subset_indices = list(range(100))\n",
    "dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "# ----- 3. Extended Transformations -----\n",
    "def get_transform(name):\n",
    "    \"\"\"\n",
    "    Each transform operates on a PIL image, and we reapply ToTensor + Normalize at the end.\n",
    "    The bug in your code: noise transform was being applied on PIL Image â€” now fixed.\n",
    "    \"\"\"\n",
    "    aug = None\n",
    "    if name == \"small_shift\":\n",
    "        aug = transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n",
    "    elif name == \"flip\":\n",
    "        aug = transforms.RandomHorizontalFlip(p=1.0)\n",
    "    elif name == \"noise\":\n",
    "        # Noise must be added on tensor, not PIL\n",
    "        def add_noise(x):\n",
    "            x = TF.to_tensor(x)\n",
    "            x = torch.clamp(x + 0.01 * torch.randn_like(x), 0, 1)\n",
    "            return x\n",
    "        return transforms.Compose([\n",
    "            transforms.Lambda(add_noise),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif name.startswith(\"rotate_\"):\n",
    "        deg = int(name.split(\"_\")[1])\n",
    "        aug = transforms.RandomRotation(degrees=deg)\n",
    "    elif name.startswith(\"scale_\"):\n",
    "        scale = float(name.split(\"_\")[1])\n",
    "        aug = transforms.Compose([\n",
    "            transforms.Resize(int(224 * scale)),\n",
    "            transforms.CenterCrop(224)\n",
    "        ])\n",
    "    else:\n",
    "        aug = transforms.Lambda(lambda x: x)\n",
    "\n",
    "    # âœ… Always reapply ToTensor and Normalize for model input\n",
    "    return transforms.Compose([\n",
    "        aug,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_names = [\n",
    "    \"small_shift\", \"flip\", \"noise\",\n",
    "    \"rotate_10\", \"rotate_20\", \"rotate_30\", \"rotate_45\",\n",
    "    \"scale_0.8\", \"scale_1.2\", \"scale_1.5\"\n",
    "]\n",
    "\n",
    "# ----- 4. Invariance Evaluation -----\n",
    "@torch.no_grad()\n",
    "def invariance_score(model, loader, transform_name):\n",
    "    transform = get_transform(transform_name)\n",
    "    total_cosine = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for imgs, _ in tqdm(loader, desc=f\"{transform_name}\"):\n",
    "        transformed_imgs = []\n",
    "        for img in imgs:\n",
    "            # Convert normalized tensor -> unnormalized PIL\n",
    "            img = img.cpu() * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "            img = img + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "            img_pil = TF.to_pil_image(img.clamp(0,1))\n",
    "            transformed_imgs.append(transform(img_pil))\n",
    "\n",
    "        imgs_t = torch.stack(transformed_imgs)\n",
    "        imgs, imgs_t = imgs.to(device), imgs_t.to(device)\n",
    "\n",
    "        out1 = F.softmax(model(imgs), dim=1)\n",
    "        out2 = F.softmax(model(imgs_t), dim=1)\n",
    "\n",
    "        cos = F.cosine_similarity(out1, out2).mean().item()\n",
    "        total_cosine += cos\n",
    "        count += 1\n",
    "\n",
    "    return total_cosine / count if count > 0 else 0.0\n",
    "\n",
    "# ----- 5. Run all experiments -----\n",
    "results = []\n",
    "for name, model in models_dict.items():\n",
    "    row = {'Model': name}\n",
    "    for tname in transform_names:\n",
    "        score = invariance_score(model, dataloader, tname)\n",
    "        row[tname] = round(score * 100, 2)\n",
    "        print(f\"{name} - {tname}: {row[tname]}% similarity\")\n",
    "    results.append(row)\n",
    "\n",
    "# ----- 6. Summary -----\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Invariance Similarity Summary (Cosine %):\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9382ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>small_shift</th>\n",
       "      <th>flip</th>\n",
       "      <th>noise</th>\n",
       "      <th>rotate_10</th>\n",
       "      <th>rotate_20</th>\n",
       "      <th>rotate_30</th>\n",
       "      <th>rotate_45</th>\n",
       "      <th>scale_0.8</th>\n",
       "      <th>scale_1.2</th>\n",
       "      <th>scale_1.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>80.71</td>\n",
       "      <td>88.25</td>\n",
       "      <td>42.16</td>\n",
       "      <td>44.19</td>\n",
       "      <td>30.94</td>\n",
       "      <td>22.70</td>\n",
       "      <td>13.32</td>\n",
       "      <td>46.68</td>\n",
       "      <td>80.21</td>\n",
       "      <td>48.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlexNet</td>\n",
       "      <td>62.97</td>\n",
       "      <td>90.07</td>\n",
       "      <td>54.07</td>\n",
       "      <td>45.89</td>\n",
       "      <td>31.44</td>\n",
       "      <td>21.61</td>\n",
       "      <td>19.71</td>\n",
       "      <td>16.75</td>\n",
       "      <td>79.66</td>\n",
       "      <td>60.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  small_shift   flip  noise  rotate_10  rotate_20  rotate_30  \\\n",
       "0  ResNet50        80.71  88.25  42.16      44.19      30.94      22.70   \n",
       "1   AlexNet        62.97  90.07  54.07      45.89      31.44      21.61   \n",
       "\n",
       "   rotate_45  scale_0.8  scale_1.2  scale_1.5  \n",
       "0      13.32      46.68      80.21      48.49  \n",
       "1      19.71      16.75      79.66      60.23  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
